#!/usr/bin/env python3
"""
å®Œæ•´çš„æ•°æ®åŠ è½½æµ‹è¯•ä»£ç  - çœŸå®è®­ç»ƒåœºæ™¯
æ¨¡æ‹ŸçœŸå®çš„è®­ç»ƒå¾ªç¯ï¼ŒåŒ…æ‹¬æ•°æ®åŠ è½½ã€æ‰¹æ¬¡å¤„ç†ã€æ€§èƒ½ç»Ÿè®¡ç­‰
"""

import sys
import os
sys.path.append('src')

import argparse
import torch
import time
import numpy as np
from collections import defaultdict
from src.data.dataset import get_model_dataset
from src.data.data_loader import get_data_loaders
from src.data.transforms import get_default_transform

class TrainingStats:
    """è®­ç»ƒç»Ÿè®¡ç±»"""
    
    def __init__(self):
        self.reset()
    
    def reset(self):
        self.batch_times = []
        self.batch_sizes = []
        self.protein_atom_counts = []
        self.ligand_atom_counts = []
        self.success_count = 0
        self.error_count = 0
        self.start_time = None
        self.end_time = None
    
    def add_batch(self, batch, batch_time, success=True):
        if success:
            self.success_count += 1
            self.batch_times.append(batch_time)
            
            if batch is not None:
                if hasattr(batch, 'num_graphs'):
                    self.batch_sizes.append(batch.num_graphs)
                
                if hasattr(batch, 'protein_pos') and batch.protein_pos is not None:
                    self.protein_atom_counts.append(batch.protein_pos.shape[0])
                
                if hasattr(batch, 'ligand_pos') and batch.ligand_pos is not None:
                    self.ligand_atom_counts.append(batch.ligand_pos.shape[0])
        else:
            self.error_count += 1
    
    def get_stats(self):
        if not self.batch_times:
            return {
                'total_batches': self.success_count + self.error_count,
                'success_count': self.success_count,
                'error_count': self.error_count,
                'success_rate': 0.0,
                'avg_batch_time': 0.0,
                'total_time': 0.0
            }
        
        return {
            'total_batches': self.success_count + self.error_count,
            'success_count': self.success_count,
            'error_count': self.error_count,
            'success_rate': self.success_count / (self.success_count + self.error_count) * 100,
            'avg_batch_time': np.mean(self.batch_times),
            'total_time': self.end_time - self.start_time if self.end_time and self.start_time else 0,
            'avg_batch_size': np.mean(self.batch_sizes) if self.batch_sizes else 0,
            'avg_protein_atoms': np.mean(self.protein_atom_counts) if self.protein_atom_counts else 0,
            'avg_ligand_atoms': np.mean(self.ligand_atom_counts) if self.ligand_atom_counts else 0
        }

def test_dataset_loading():
    """æµ‹è¯•æ•°æ®é›†åŠ è½½"""
    print("=== æµ‹è¯•æ•°æ®é›†åŠ è½½ ===")
    
    # åˆ›å»ºé…ç½®
    config = argparse.Namespace(
        name='crossdocked',
        path='src/data/crossdocked_v1.1_rmsd1.0_processed',
        split='src/data/crossdocked_v1.1_rmsd1.0_processed/split_by_name.pt',
        mode='full',
        version='ref_prior_aromatic'
    )
    
    try:
        # åŠ è½½æ•°æ®é›†
        result = get_model_dataset(config)
        
        if isinstance(result, tuple):
            dataset, subsets = result
            print(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ")
            print(f"   æ€»æ ·æœ¬æ•°: {len(dataset)}")
            print(f"   åˆ†å‰²: {list(subsets.keys())}")
            for split_name, subset in subsets.items():
                print(f"   {split_name}: {len(subset)} ä¸ªæ ·æœ¬")
        else:
            dataset = result
            print(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ (æ— åˆ†å‰²)")
            print(f"   æ€»æ ·æœ¬æ•°: {len(dataset)}")
        
        # æµ‹è¯•å•ä¸ªæ ·æœ¬
        print("\n--- æµ‹è¯•å•ä¸ªæ ·æœ¬ ---")
        sample = dataset[0]
        if sample is not None:
            print("âœ… å•ä¸ªæ ·æœ¬åŠ è½½æˆåŠŸ")
            print(f"   æ ·æœ¬ID: {getattr(sample, 'id', 'N/A')}")
            if hasattr(sample, 'protein_pos') and sample.protein_pos is not None:
                print(f"   è›‹ç™½è´¨åŸå­æ•°: {sample.protein_pos.shape[0]}")
            if hasattr(sample, 'ligand_pos') and sample.ligand_pos is not None:
                print(f"   é…ä½“åŸå­æ•°: {sample.ligand_pos.shape[0]}")
        else:
            print("âŒ å•ä¸ªæ ·æœ¬åŠ è½½å¤±è´¥")
        
        return dataset, subsets if isinstance(result, tuple) else None
        
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None

def test_data_loader():
    """æµ‹è¯•æ•°æ®åŠ è½½å™¨"""
    print("\n=== æµ‹è¯•æ•°æ®åŠ è½½å™¨ ===")
    
    try:
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        data_loaders = get_data_loaders(
            dataset_path='src/data/crossdocked_v1.1_rmsd1.0_processed',
            batch_size=32,
            num_workers=0,  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
            split_file='src/data/crossdocked_v1.1_rmsd1.0_processed/split_by_name.pt',
            shuffle_train=True,
            shuffle_test=False
        )
        
        print(f"âœ… æ•°æ®åŠ è½½å™¨åˆ›å»ºæˆåŠŸ")
        print(f"   å¯ç”¨çš„åˆ†å‰²: {list(data_loaders.keys())}")
        
        # æµ‹è¯•æ¯ä¸ªåˆ†å‰²çš„åŠ è½½å™¨
        for split_name, loader in data_loaders.items():
            print(f"\n--- æµ‹è¯• {split_name} åŠ è½½å™¨ ---")
            print(f"   æ‰¹æ¬¡æ•°é‡: {len(loader)}")
            
            # æµ‹è¯•ç¬¬ä¸€ä¸ªæ‰¹æ¬¡
            try:
                for batch_idx, batch in enumerate(loader):
                    if batch is not None:
                        print(f"âœ… æ‰¹æ¬¡ {batch_idx} åŠ è½½æˆåŠŸ")
                        print(f"   æ‰¹æ¬¡å¤§å°: {batch.num_graphs if hasattr(batch, 'num_graphs') else 'N/A'}")
                        if hasattr(batch, 'protein_pos') and batch.protein_pos is not None:
                            print(f"   è›‹ç™½è´¨åŸå­æ€»æ•°: {batch.protein_pos.shape[0]}")
                        if hasattr(batch, 'ligand_pos') and batch.ligand_pos is not None:
                            print(f"   é…ä½“åŸå­æ€»æ•°: {batch.ligand_pos.shape[0]}")
                    else:
                        print(f"âŒ æ‰¹æ¬¡ {batch_idx} åŠ è½½å¤±è´¥")
                    break  # åªæµ‹è¯•ç¬¬ä¸€ä¸ªæ‰¹æ¬¡
            except Exception as e:
                print(f"âŒ {split_name} åŠ è½½å™¨æµ‹è¯•å¤±è´¥: {e}")
        
        return data_loaders
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å™¨åˆ›å»ºå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None

def test_transforms():
    """æµ‹è¯•æ•°æ®å˜æ¢"""
    print("\n=== æµ‹è¯•æ•°æ®å˜æ¢ ===")
    
    try:
        # åˆ›å»ºå˜æ¢
        transform = get_default_transform(
            add_ord_feat=False,
            ligand_atom_mode='basic',
            ligand_bond_mode='fc',
            max_num_arms=10,
            random_rot=False,  # å…³é—­éšæœºæ—‹è½¬ä»¥ä¾¿æµ‹è¯•
            normalize_pos=True,
            add_noise=False   # å…³é—­å™ªå£°ä»¥ä¾¿æµ‹è¯•
        )
        print("âœ… å˜æ¢åˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        from src.data.utils import ProteinLigandData
        test_data = ProteinLigandData()
        test_data.protein_pos = torch.randn(10, 3)
        test_data.ligand_pos = torch.randn(5, 3)
        test_data.protein_atom_feature = torch.randn(10, 9)
        test_data.ligand_atom_feature = torch.randn(5, 9)
        test_data.ligand_bond_index = torch.randint(0, 5, (2, 8))
        test_data.ligand_bond_type = torch.randint(1, 4, (8,))
        
        print("âœ… æµ‹è¯•æ•°æ®åˆ›å»ºæˆåŠŸ")
        print(f"   å˜æ¢å‰ - è›‹ç™½è´¨åŸå­æ•°: {test_data.protein_pos.shape[0]}")
        print(f"   å˜æ¢å‰ - é…ä½“åŸå­æ•°: {test_data.ligand_pos.shape[0]}")
        
        # åº”ç”¨å˜æ¢
        transformed_data = transform(test_data)
        print("âœ… å˜æ¢åº”ç”¨æˆåŠŸ")
        print(f"   å˜æ¢å - è›‹ç™½è´¨åŸå­æ•°: {transformed_data.protein_pos.shape[0]}")
        print(f"   å˜æ¢å - é…ä½“åŸå­æ•°: {transformed_data.ligand_pos.shape[0]}")
        
        return True
        
    except Exception as e:
        print(f"âŒ å˜æ¢æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_real_training_scenario():
    """æµ‹è¯•çœŸå®è®­ç»ƒåœºæ™¯"""
    print("\n=== æµ‹è¯•çœŸå®è®­ç»ƒåœºæ™¯ ===")
    
    # è®­ç»ƒé…ç½®
    TRAINING_CONFIG = {
        'batch_size': 32,
        'num_workers': 0,
        'test_batches': 50,  # æµ‹è¯•çš„æ‰¹æ¬¡æ•°é‡
    }
    
    print("ğŸ“Š è®­ç»ƒé…ç½®:")
    for key, value in TRAINING_CONFIG.items():
        print(f"   {key}: {value}")
    
    try:
        # è·å–æ•°æ®åŠ è½½å™¨
        data_loaders = get_data_loaders(
            dataset_path='src/data/crossdocked_v1.1_rmsd1.0_processed',
            batch_size=TRAINING_CONFIG['batch_size'],
            num_workers=TRAINING_CONFIG['num_workers'],
            split_file='src/data/crossdocked_v1.1_rmsd1.0_processed/split_by_name.pt',
            shuffle_train=True,
            shuffle_test=False
        )
        
        print(f"âœ… çœŸå®è®­ç»ƒDataLoaderåˆ›å»ºæˆåŠŸ")
        
        # æµ‹è¯•è®­ç»ƒé›†
        if 'train' in data_loaders:
            train_stats = test_loader_performance(
                data_loaders['train'], 
                'TRAIN', 
                TRAINING_CONFIG['test_batches']
            )
            
            print(f"\nğŸ“Š TRAIN æ€§èƒ½ç»Ÿè®¡:")
            stats = train_stats.get_stats()
            for key, value in stats.items():
                if isinstance(value, float):
                    print(f"   {key}: {value:.3f}")
                else:
                    print(f"   {key}: {value}")
        
        # æµ‹è¯•æµ‹è¯•é›†
        if 'test' in data_loaders:
            test_stats = test_loader_performance(
                data_loaders['test'], 
                'TEST', 
                min(20, TRAINING_CONFIG['test_batches']//2)
            )
            
            print(f"\nğŸ“Š TEST æ€§èƒ½ç»Ÿè®¡:")
            stats = test_stats.get_stats()
            for key, value in stats.items():
                if isinstance(value, float):
                    print(f"   {key}: {value:.3f}")
                else:
                    print(f"   {key}: {value}")
        
        return True
        
    except Exception as e:
        print(f"âŒ çœŸå®è®­ç»ƒåœºæ™¯æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_loader_performance(dataloader, split_name, total_batches):
    """æµ‹è¯•æ•°æ®åŠ è½½å™¨æ€§èƒ½"""
    stats = TrainingStats()
    stats.start_time = time.time()
    
    print(f"\nğŸ“ˆ æµ‹è¯•{split_name} DataLoader:")
    print(f"   {split_name}: æµ‹è¯• {total_batches} ä¸ªæ‰¹æ¬¡")
    
    batch_count = 0
    
    for batch_idx, batch in enumerate(dataloader):
        if batch_idx >= total_batches:
            break
        
        batch_start = time.time()
        
        try:
            # æ¨¡æ‹ŸçœŸå®è®­ç»ƒä¸­çš„æ‰¹æ¬¡å¤„ç†
            if batch is not None:
                # æ¨¡æ‹Ÿå‰å‘ä¼ æ’­
                if hasattr(batch, 'protein_pos') and batch.protein_pos is not None:
                    # æ¨¡æ‹Ÿå¯¹è›‹ç™½è´¨ä½ç½®çš„å¤„ç†
                    protein_center = batch.protein_pos.mean(dim=0, keepdim=True)
                
                if hasattr(batch, 'ligand_pos') and batch.ligand_pos is not None:
                    # æ¨¡æ‹Ÿå¯¹é…ä½“ä½ç½®çš„å¤„ç†
                    ligand_center = batch.ligand_pos.mean(dim=0, keepdim=True)
                
                # æ¨¡æ‹ŸæŸå¤±è®¡ç®—ï¼ˆç®€å•ç¤ºä¾‹ï¼‰
                if hasattr(batch, 'protein_pos') and hasattr(batch, 'ligand_pos'):
                    # è®¡ç®—è›‹ç™½è´¨å’Œé…ä½“ä¸­å¿ƒçš„è·ç¦»ä½œä¸ºç¤ºä¾‹
                    if batch.protein_pos.size(0) > 0 and batch.ligand_pos.size(0) > 0:
                        distance = torch.norm(protein_center - ligand_center)
                
                stats.add_batch(batch, time.time() - batch_start, success=True)
            else:
                stats.add_batch(None, time.time() - batch_start, success=False)
            
            batch_count += 1
            
            # æ¯10ä¸ªæ‰¹æ¬¡æŠ¥å‘Šä¸€æ¬¡è¿›åº¦
            if batch_count % 10 == 0:
                elapsed = time.time() - stats.start_time
                avg_time = elapsed / batch_count
                print(f"     æ‰¹æ¬¡ {batch_count}/{total_batches}: æˆåŠŸ={stats.success_count}, é”™è¯¯={stats.error_count}, å¹³å‡æ—¶é—´={avg_time:.3f}s")
        
        except Exception as e:
            stats.add_batch(None, time.time() - batch_start, success=False)
            batch_count += 1
            if batch_count <= 5:  # åªæ‰“å°å‰5ä¸ªé”™è¯¯
                print(f"     æ‰¹æ¬¡ {batch_count} å¤„ç†å¤±è´¥: {e}")
    
    stats.end_time = time.time()
    
    # æ‰“å°æœ€ç»ˆç»Ÿè®¡
    elapsed = stats.end_time - stats.start_time
    print(f"   Epoch 1 å®Œæˆ: {elapsed:.2f}s, {batch_count} æ‰¹æ¬¡")
    
    return stats

def test_full_pipeline():
    """æµ‹è¯•å®Œæ•´æµç¨‹"""
    print("=== æµ‹è¯•å®Œæ•´æµç¨‹ ===")
    
    try:
        # 1. åŠ è½½æ•°æ®é›†
        dataset, subsets = test_dataset_loading()
        if dataset is None:
            return False
        
        # 2. åˆ›å»ºæ•°æ®åŠ è½½å™¨
        data_loaders = test_data_loader()
        if data_loaders is None:
            return False
        
        # 3. æµ‹è¯•å˜æ¢
        if not test_transforms():
            return False
        
        # 4. æµ‹è¯•çœŸå®è®­ç»ƒåœºæ™¯
        if not test_real_training_scenario():
            return False
        
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å®Œæ•´æµç¨‹å·¥ä½œæ­£å¸¸ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ å®Œæ•´æµç¨‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¼€å§‹å®Œæ•´çš„æ•°æ®åŠ è½½æµ‹è¯•...")
    
    success = test_full_pipeline()
    
    if success:
        print("\nâœ… æµ‹è¯•å®Œæˆï¼æ‰€æœ‰åŠŸèƒ½æ­£å¸¸ï¼")
        print("ğŸ¯ ç³»ç»Ÿå·²å‡†å¤‡å¥½è¿›è¡ŒçœŸå®è®­ç»ƒï¼")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼éœ€è¦è°ƒè¯•ï¼")
    
    return success

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nâ¹ï¸ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()